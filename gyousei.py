# -*- coding: utf-8 -*-
"""gyousei.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bf8rjsxFCNcctnfrIVBuH1SO2E8I2l-E
"""

from google.colab import drive
# === Cell 2: mount & locate ===
import os, sys, unicodedata, re
from pathlib import Path
from google.colab import drive
import os
from getpass import getpass
from openai import OpenAI
import fitz  # PyMuPDF
import os, json, re, time, unicodedata
from pathlib import Path
from typing import List, Tuple, Dict, Any
import fitz  # PyMuPDF
from google.colab import drive
from time import sleep


drive.mount('/content/drive')
os.listdir('/content/drive/Shareddrives/ぎょうせい')
# Colab 環境変数: open_ai_key または OPENAI_API_KEY を採用
api_key = os.environ.get("openai_api_key")
if not api_key:
    api_key = getpass("Enter your OPENAI_API_KEY (非表示): ")
os.environ["OPENAI_API_KEY"] = api_key  # SDK は OPENAI_API_KEY を既定で参照します :contentReference[oaicite:1]{index=1}

client = OpenAI()  # Responses API が既定の一次 API。Chat Completions も引き続き利用可能です。:contentReference[oaicite:2]{index=2}
MODEL = "gpt-4o-mini"  # コスト効率重視。必要なら gpt-4o 等に変更可。:contentReference[oaicite:3]{index=3}
API_KEY_ENV_CANDIDATES = ["open_ai_key", "OPENAI_API_KEY"]


"""
骨太戦略のpdfを読み、KPIを抽出する
"""

RAW_PATH = '/content/drive/Shareddrives/ぎょうせい/骨太.pdf'
def normalize_path(p: str) -> str:
    # Mac/Drive 由来の結合文字問題を緩和
    return unicodedata.normalize('NFC', p)

def file_exists(p: str) -> bool:
    return Path(p).exists()

def find_pdf_by_name(root='/content/drive/Shareddrives', target_name='骨太.pdf'):
    root_path = Path(root)
    if not root_path.exists():
        return None
    tn_norm = unicodedata.normalize('NFKC', target_name).casefold()
    for p in root_path.rglob('*.pdf'):
        name_norm = unicodedata.normalize('NFKC', p.name).casefold()
        if name_norm == tn_norm or ('骨太' in name_norm and name_norm.endswith('.pdf')):
            return str(p)
    return None
def extract_pages(pdf_path: str):
    texts = []
    with fitz.open(pdf_path) as doc:
        for i, page in enumerate(doc, start=1):
            txt = page.get_text("text")
            # 文字が少なすぎる（スキャン画像等）の場合は空文字のまま残す
            texts.append((i, txt.strip()))
    return texts


CANDIDATE_PATH = normalize_path(RAW_PATH)
pages = extract_pages(CANDIDATE_PATH)
total_chars = sum(len(t) for _, t in pages)
print(f"[ok] ページ数: {len(pages)}, 文字総数(概算): {total_chars}")
# === Cell 5: chunking ===
def make_chunks(pages, max_chars=8000):
    chunks = []
    buf, p_start = "", None
    for pno, text in pages:
        if p_start is None:
            p_start = pno
        # ページ区切りを残してページ範囲をあとで表示できるようにする
        candidate = buf + f"\n\n# [Page {pno}]\n" + text
        if len(candidate) > max_chars and buf:
            chunks.append((p_start, pno-1, buf.strip()))
            buf = f"# [Page {pno}]\n{text}"
            p_start = pno
        else:
            buf = candidate
    if buf.strip():
        p_end = pages[-1][0] if p_start is not None else None
        chunks.append((p_start, p_end, buf.strip()))
    return chunks

chunks = make_chunks(pages, max_chars=8000)  # 文字ベース近似（gpt-4o-mini は広いコンテキストに対応）
print(f"[ok] チャンク数: {len(chunks)}")
for i,(ps,pe,_) in enumerate(chunks[:3],1):
    print(f"  - chunk{i}: pages {ps}-{pe}")

def summarize_chunk(text_chunk: str, page_span: str) -> str:
    # Responses API でシンプルに実行（json 要求はモデル柔軟性に依存するため文字列出力で取得）
    prompt = (
    "あなたは日本の政策文書に精通した医療従事者であり、専門的な要約者です。"
    "以下に示すのは同一PDFから抽出した一部テキストです。"
    "この内容をもとに、以下の観点から体系的に整理してください：\n"
    "1. 重要方針（全体的な戦略目標）\n"
    "2. 具体施策（制度改革・実行計画・担当主体を含む）\n"
    "3. KPIや数値目標（期限・定量指標があれば必ず明記）\n"
    "4. 予算・財政上の含意（資源配分、財政制約）\n"
    "5. 規制・制度改正に関する要素\n"
    "6. 医療・福祉・産業への影響\n"
    "7. 未解決課題・リスク・トレードオフ\n"
    "8. 関連する省庁や主要アクター\n\n"
    "さらに、これらを踏まえて、今後の医療政策決定において重視すべきKPIを網羅的に抽出してください。"
    "KPIは、次の二種類に分けて整理してください：\n"
    "A. 定量的に評価可能なもの（例：GDP増大、平均寿命、医療費対GDP比、医療従事者数など）\n"
    "B. 定量化が困難なもの（例：医療従事者の幸福度、患者満足度、医療アクセスの公平性など）\n\n"
    "出力形式は箇条書きを中心にし、冗長な重複は統合してください。\n"
    f"【対象ページ】{page_span}\n"
    "【テキスト】\n"
    f"{text_chunk}\n"
  )
    resp = client.responses.create(
        model=MODEL,
        input=prompt,
        temperature=0.2,
    )
    return resp.output_text.strip()

intermediate = []
for idx, (ps, pe, body) in enumerate(chunks, 1):
    page_span = f"p.{ps}-{pe}"
    #print(f"[run] chunk {idx}/{len(chunks)} {page_span} を要約中...")
    s = summarize_chunk(body, page_span)
    intermediate.append(f"## {page_span}\n{s}")
    sleep(0.1)

import re
from typing import List, Dict, Any

def extract_kpi_sections(text: str) -> Dict[str, Any]:
    """
    1つのドキュメント文字列から、
    '#### A. 定量的に評価可能なもの' と
    '#### B. 定量化が困難なもの' 直後の本文（箇条書き等）だけを抽出する。

    返り値:
      {
        "page": "<先頭の '## p.xxx' 見出し（なければ None）>",
        "A_raw": "<A セクション本文（トリム済み） or ''>",
        "B_raw": "<B セクション本文（トリム済み） or ''>",
        "A_items": ["- ...","- ..."],  # 箇条書きを1行1要素で正規化（先頭の-や**は除去）
        "B_items": ["- ...","- ..."],
      }
    """
    # ページ見出し（例: "## p.1-9"）を取得
    m_page = re.search(r'^\s*##\s*p\.[^\n]+', text, flags=re.IGNORECASE | re.MULTILINE)
    page = m_page.group(0).strip() if m_page else None

    # #### 見出しブロックを章単位で拾う
    # A と B それぞれ、次の #### か 文末 までを非貪欲に取る
    # （水平線や空行2つ以上でセクション終端になるケースにも配慮）
    def grab_after(heading_pattern: str) -> str:
        # 次の #### まで or 文末まで
        # DOTALL で改行も含め、最短一致 (?=####|\Z) で先読み切断
        pat = rf'{heading_pattern}\s*\n(?P<body>.*?)(?=\n####\s|^\s*---\s*$|\Z)'
        m = re.search(pat, text, flags=re.DOTALL | re.MULTILINE)
        return (m.group('body').strip() if m else "")

    A_body = grab_after(r'####\s*A\.\s*定量的に評価可能なもの')
    B_body = grab_after(r'####\s*B\.\s*定量化が困難なもの')

    # 箇条書きの各行をアイテムに正規化
    # 先頭の "-", "・", "—" などや "**" を除去し、空行は落とす
    def body_to_items(body: str) -> List[str]:
        items = []
        for line in body.splitlines():
            line = line.strip()
            if not line:
                continue
            # 箇条書き行のみ拾う（"- " で始まる想定だが、太字・記号付きも許容）
            if re.match(r'^[-•・—–]\s+', line):
                clean = re.sub(r'^[-•・—–]\s+', '', line)
                clean = clean.strip()
                # Markdownの**強調**や全角記号のノイズを軽く除去
                clean = re.sub(r'^\*+\s*', '', clean)        # 先頭の*や**を除去
                clean = re.sub(r'\s*\*+\s*$', '', clean)     # 末尾の*や**を除去
                clean = re.sub(r'^\*\*|\*\*$', '', clean)    # 両端の**を除去
                items.append(clean)
            else:
                # 箇条書き記号がなくても、非空行は本文として採用したい場合は以下を有効化
                # items.append(line)
                pass
        # 箇条書きが検出できなかった場合、本文全体を1要素として返す（保険）
        if not items and body:
            items = [re.sub(r'\s+', ' ', body)]
        return items

    A_items = body_to_items(A_body)
    B_items = body_to_items(B_body)

    return {
        "page": page,
        "A_raw": A_body,
        "B_raw": B_body,
        "A_items": A_items,
        "B_items": B_items,
    }

def extract_from_intermediate(intermediate: List[str]) -> Dict[str, Any]:
    """
    intermediate（文字列リスト）全体を処理して、
    各要素の抽出結果と、A/Bの集計リストを返す。
    """
    results = []
    all_A, all_B = [], []
    for doc in intermediate:
        res = extract_kpi_sections(doc)
        results.append(res)
        all_A.extend(res["A_items"])
        all_B.extend(res["B_items"])
    return {
        "per_doc": results,
        "A_aggregate": all_A,
        "B_aggregate": all_B,
    }

intermediate = extract_from_intermediate(intermediate)

"""
RAからグラフの抽出
"""
TEMPERATURE = 0.0
SLEEP_BETWEEN_CALLS=0.1
MAX_CHARS_PER_CHUNK=8000

# ---- Paths ----
RS_FORMAT_PDF = "/content/drive/Shareddrives/ぎょうせい/RS_format.pdf"
SET_B_DIR     = "/content/drive/Shareddrives/ぎょうせい/セットB: 働き方改革"
OUT_DIR       = "/content/drive/Shareddrives/ぎょうせい/セットB: 働き方改革/rs_outputs"


# ユーティリティ：Responses API 呼び出し（テキスト→テキスト）
def call_gpt(prompt: str, *, system: str = None, temperature: float = TEMPERATURE) -> str:
    # Responses APIの素直な呼び出し。:contentReference[oaicite:2]{index=2}
    if system:
        # instructions or system-like前置き
        resp = client.responses.create(
            model=MODEL,
            instructions=system,
            input=prompt,
            temperature=temperature,
        )
    else:
        resp = client.responses.create(
            model=MODEL,
            input=prompt,
            temperature=temperature,
        )
    return resp.output_text.strip()

# =========================
# Cell 3) PDF抽出ユーティリティ
# =========================


def extract_pdf_text(pdf_path: str) -> List[Tuple[int, str]]:
    """
    Returns: [(page_number (1-origin), text), ...]
    ページ区切りを保持して全文抽出。スキャンPDFの場合 text が空に近くなることがあります。
    """
    pages = []
    with fitz.open(pdf_path) as doc:
        for i, page in enumerate(doc, start=1):
            txt = page.get_text("text")
            pages.append((i, (txt or "").strip()))
    return pages

def join_with_page_markers(pages: List[Tuple[int, str]]) -> str:
    """
    [Page X] の区切りを明示した単一文字列を返す（モデルへの投入用）。
    """
    buf = []
    for pno, txt in pages:
        buf.append(f"[Page {pno}]\n{txt}")
    return "\n\n".join(buf)

def make_chunks(full_text: str, max_chars: int = MAX_CHARS_PER_CHUNK) -> List[Tuple[int, int, str]]:
    """
    文字数ベース近似のチャンク分割。
    戻り値: [(start_marker_index, end_marker_index, chunk_text)]
      ここではページ番号ではなく便宜的な連番。ただしチャンク内部に [Page X] が含まれる。
    """
    chunks = []
    cur = []
    cur_len = 0
    start_idx = 1
    parts = full_text.split("\n\n[Page ")
    # 先頭の "[Page " を保持しやすくするため復元
    # 例: ["[Page 1]\n...", "2]\n...", "3]\n..."]
    if parts and not parts[0].startswith("[Page "):
        parts[0] = parts[0]  # 先頭に既に "[Page" がある想定
    reconstructed = []
    if parts:
        reconstructed.append(parts[0])
        for tail in parts[1:]:
            reconstructed.append("[Page " + tail)
    else:
        reconstructed = [full_text]

    for i, seg in enumerate(reconstructed, start=1):
        if cur_len + len(seg) > max_chars and cur:
            chunk_text = "\n\n".join(cur).strip()
            chunks.append((start_idx, i-1, chunk_text))
            cur = [seg]
            cur_len = len(seg)
            start_idx = i
        else:
            cur.append(seg)
            cur_len += len(seg)
    if cur:
        chunk_text = "\n\n".join(cur).strip()
        chunks.append((start_idx, start_idx + len(cur) - 1, chunk_text))
    return chunks

def ensure_outdir(path: str):
    Path(path).mkdir(parents=True, exist_ok=True)

# JSON安全パーサ（モデル出力のJSON限定を想定しつつ頑健化）
def safe_json_loads(s: str) -> Any:
    s = s.strip()
    # すでにJSONならそのまま
    try:
        return json.loads(s)
    except Exception:
        pass
    # 本文中に最初の { ～ 最後の } を抽出
    m1 = s.find("{")
    m2 = s.rfind("}")
    if m1 != -1 and m2 != -1 and m2 > m1:
        candidate = s[m1:m2+1]
        try:
            return json.loads(candidate)
        except Exception:
            # 末尾カンマ・コメント等の簡易修復は最小限に止める
            candidate = re.sub(r",\s*([}\]])", r"\1", candidate)
            return json.loads(candidate)
    raise ValueError("JSONとして解釈できませんでした。")



RS_FORMAT_PDF = normalize_path(RS_FORMAT_PDF)
rs_pages = extract_pdf_text(RS_FORMAT_PDF)
rs_full_text = join_with_page_markers(rs_pages)

primer_prompt = (
    "あなたは日本の行政文書（レビューシート/効果発現モデル）の専門家である。"
    "以下の資料（RS_format.pdfの全文）を読み、"
    "1)『レビューシートとは何か』、2)『効果発現モデルとは何か』を、"
    "実務者向けに簡潔かつ厳密に定義し、構成要素・最低限含めるべき項目・よくある表記ゆらぎ（同義語）を列挙した"
    "**JSONのみ**を返せ。説明文やコードフェンスは一切出力しない。"
    "出力スキーマは以下に厳密準拠："
    "{"
    "\"what_is_review_sheet\": {\"definition\": \"\", \"must_include\": [], \"synonyms\": []},"
    "\"what_is_effect_model\": {\"definition\": \"\", \"must_include\": [], \"synonyms\": []}"
    "}"
    "\n\n【RS_format全文】\n"
    f"{rs_full_text}\n"
)

primer_text= call_gpt(primer_prompt, temperature=0.0)
primer = safe_json_loads(primer_text)
print("=== PRIMER (抜粋) ===")
print(primer)

# Map段：各チャンクから候補を抽出（概要・目的の候補、効果発現経路の部分グラフ）
MAP_PROMPT_TMPL = """
あなたは日本の行政レビューシートの情報抽出エンジンである。
以下の事前知識（プライマー）を前提に、与えられたPDFテキスト断片（同一文書の一部）から、
1) 「概要・目的」セクション本文の候補（原文抜粋、要約禁止）を抽出、
2) 「効果発現経路（ロジックモデル）」に該当する因果要素（Activity→Output→Outcome(short/mid/long)）の**部分**を抽出し、
厳密なJSONのみを返せ。余計な文字は出力しない。

【事前知識（抜粋）】
- レビューシート: {primer_review_def}
- 効果発現モデル: {primer_effect_def}
- 用語の同義・ゆらぎ: ReviewSheetSyn={primer_review_syn}, EffectModelSyn={primer_effect_syn}

【抽出ルール】
- 見出しゆらぎ（例: 概要/目的/背景・目的/狙い/ねらい/概要・目的 など）に寛容。
- 「効果発現経路」「成果連鎖」「ロジックモデル」「アウトカムパス」「因果経路」等を同義として扱う。
- 箇条書き・図表・本文叙述いずれでも可。図の場合は読み取れるテキストを優先。
- ノードは名詞句の核で正規化し、重複は統合（ただしMap段では候補として重複容認）。
- ページ根拠を `evidence.page`（例: "p12"）と `evidence.quote` に可能な範囲で付す。
- JSONのみを返す。スキーマは以下。

JSONスキーマ:
{{
  "overview_candidates": [
    {{"page": "pX", "text": "<原文抜粋>"}}
  ],
  "graph_candidates": {{
    "activities": [{{"label": "<活動>","page":"pX"}}],
    "outputs":    [{{"label": "<アウトプット>","page":"pX"}}],
    "outcomes": {{
      "short": [{{"label": "<短期>","page":"pX"}}],
      "mid":   [{{"label": "<中期>","page":"pX"}}],
      "long":  [{{"label": "<長期>","page":"pX"}}]
    }},
    "edges": [
      {{"from": "<label>", "to": "<label>", "type": "activity->output|output->short|short->mid|mid->long",
        "evidence": {{"page":"pX","quote":"<根拠抜粋>"}}}}
    ]
  }},
  "pages_seen": ["pX","pY"]
}}

【対象ファイル】{file_path}
【チャンク範囲】{chunk_span}
【テキスト断片】:
{chunk_text}
"""

# Reduce段：全チャンクの候補を統合 → 最終スキーマで返す
REDUCE_PROMPT_TMPL = """
あなたは日本の行政レビューシートの統合器である。
以下は同一PDFの各チャンクから抽出された候補JSONの配列である。重複や同義を統合し、
最終スキーマ（厳格JSON）で出力せよ。余計な文字は一切出さない。

統合規則：
- overview_candidates の本文は、最も適合度の高い1件を選択（該当なければ空文字）。
- ノード統合：全角半角・表記ゆらぎ・助詞/装飾語を正規化してラベル重複を統合。
- edges は存在する根拠に基づき、Activity→Output→Outcome(short→mid→long) の鎖が最大になるよう接続。
- pages_cited は根拠ページのユニーク集合。
- スキャン等で復元困難なら flags.incomplete_graph=true。
- 概要・目的が見当たらなければ flags.missing_overview_purpose=true。

最終スキーマ:
{{
  "skip": false,
  "file_path": "{file_path}",
  "title": "<推定タイトル(本文先頭の大見出し等から推定。無ければ空)>",
  "matched_RS": true,
  "overview_purpose": "<本文抜粋 or 空>",
  "effect_pathway": {{
    "activities": [{{"id":"A1","label":"<活動1>"}}, {{}} ],
    "outputs":    [{{"id":"O1","label":"<アウトプット1>"}}],
    "outcomes": {{
      "short": [{{"id":"S1","label":"<短期1>"}}],
      "mid":   [{{"id":"M1","label":"<中期1>"}}],
      "long":  [{{"id":"L1","label":"<長期1>"}}]
    }},
    "edges": [
      {{"from":"A1","to":"O1","type":"activity->output","evidence":{{"page":"pX","quote":"<根拠>"}}}},
      {{"from":"O1","to":"S1","type":"output->short","evidence":{{"page":"pY","quote":"<根拠>"}}}},
      {{"from":"S1","to":"M1","type":"short->mid","evidence":{{"page":"pZ","quote":"<根拠>"}}}},
      {{"from":"M1","to":"L1","type":"mid->long","evidence":{{"page":"pW","quote":"<根拠>"}}}}
    ]
  }},
  "pages_cited": ["pX","pY"],
  "confidence": 0.0,
  "flags": {{
    "missing_overview_purpose": false,
    "incomplete_graph": false,
    "needs_ocr": false
  }}
}}

【候補JSON配列】:
{map_results_json}
"""

def process_one_pdf(file_path: str, primer: Dict[str, Any]) -> Dict[str, Any]:
    file_path = normalize_path(file_path)
    pages = extract_pdf_text(file_path)

    # ほぼ空=スキャンの可能性
    total_chars = sum(len(t) for _, t in pages)
    needs_ocr = total_chars < 50

    full_text = join_with_page_markers(pages)
    chunks = make_chunks(full_text, max_chars=MAX_CHARS_PER_CHUNK)

    # Map: 各チャンク処理
    map_results = []
    for (sidx, eidx, ctext) in chunks:
        chunk_span = f"chunk{sidx}-{eidx}"
        prompt = MAP_PROMPT_TMPL.format(
            primer_review_def = primer["what_is_review_sheet"]["definition"],
            primer_effect_def = primer["what_is_effect_model"]["definition"],
            primer_review_syn = ", ".join(primer["what_is_review_sheet"].get("synonyms", [])),
            primer_effect_syn = ", ".join(primer["what_is_effect_model"].get("synonyms", [])),
            file_path=file_path,
            chunk_span=chunk_span,
            chunk_text=ctext
        )
        out = call_gpt(prompt, temperature=0.0)
        try:
            j = safe_json_loads(out)
        except Exception as e:
            # 失敗したチャンクは空壊れとしてスキップ（ロバスト化）
            j = {"overview_candidates": [], "graph_candidates": {"activities":[],"outputs":[],"outcomes":{"short":[],"mid":[],"long":[]}, "edges":[]}, "pages_seen":[]}
        map_results.append(j)
        time.sleep(SLEEP_BETWEEN_CALLS)

    # Reduce: 統合
    map_results_json = json.dumps(map_results, ensure_ascii=False)
    reduce_prompt = REDUCE_PROMPT_TMPL.format(
        file_path=file_path,
        map_results_json=map_results_json
    )
    reduced_text = call_gpt(reduce_prompt, temperature=0.0)
    final = safe_json_loads(reduced_text)

    # needs_ocr フラグ補正（元PDFが文字ほぼ無しなら立てる）
    if needs_ocr:
        final.setdefault("flags", {}).update({"needs_ocr": True})
    return final

ensure_outdir(OUT_DIR)

SET_B_DIR = normalize_path(SET_B_DIR)
pdf_files = sorted(Path(SET_B_DIR).rglob("*.pdf"))

# ファイル名に "RS"（大文字小文字区別無し）を含むもののみ対象
target_files = [str(p) for p in pdf_files if "rs" in p.name.casefold()]

results = []
for i, fp in enumerate(target_files, 1):
    print(f"[{i}/{len(target_files)}] Processing: {fp}")
    try:
        result = process_one_pdf(fp, primer)
        # RSフィルタはファイル名で済んでいるが、最終出力にも matched_RS=true を入れておく
        result["matched_RS"] = True
        results.append(result)
    except Exception as e:
        # 失敗時はスキップJSONで記録
        results.append({
            "skip": True,
            "file_path": fp,
            "reason": f"exception: {type(e).__name__}: {e}"
        })

# 保存（個別＋集約）
for r in results:
    base = Path(r.get("file_path", "unknown.pdf")).stem
    out_path = Path(OUT_DIR) / f"{base}_extracted.json"
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(r, f, ensure_ascii=False, indent=2)

index_path = Path(OUT_DIR) / "RS_batch_results.json"
with open(index_path, "w", encoding="utf-8") as f:
    json.dump({"results": results}, f, ensure_ascii=False, indent=2)
print(f"[DONE] Saved {len(results)} files. Index: {index_path}")

# ============================================================
# Colab用：KPI抽出・更新・効果発現経路生成（複数プロンプト版 / Template修正版）
# 前提：
#   - intermediate: List[str] …… 事前に用意されたテキスト断片のリスト
#   - SET_B_DIR: 「セットB: 働き方改革」ディレクトリの絶対パス
#   - RS1 をファイル名 or ディレクトリ名に含むPDFは除外して処理する
#   - OUT_DIR: 出力先ディレクトリ
# 依存：
#   pip install --upgrade openai PyMuPDF
# モデル：
#   最初は軽量確認用に gpt-4o-mini を使用（必要時 gpt-4o に切替）
# ============================================================
import os, json, re, unicodedata, time
from pathlib import Path
from typing import List, Dict, Any, Tuple
import fitz  # PyMuPDF
from google.colab import drive
from string import Template
from openai import OpenAI
from getpass import getpass

TEMPERATURE = 0.0
#MAX_CHARS_PER_CHUNK = 8000
OUT_JSON  = "RS_batch_results.json"

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
# 1) まず対象キーだけ取り出す（無ければ空リスト）
data=intermediate
a = data.get("A_aggregate", [])
b = data.get("B_aggregate", [])

# 2) リネーム＋それ以外は削除
intermediate= {
    "quantitative_kpi": a,
    "hard_to_quantify_kpi": b,
}

def nfc(p: str) -> str:
    return unicodedata.normalize("NFC", p)

def extract_pdf_pages(pdf_path: str) -> List[Tuple[int, str]]:
    pages = []
    with fitz.open(pdf_path) as doc:
        for i, page in enumerate(doc, start=1):
            txt = page.get_text("text") or ""
            pages.append((i, txt.strip()))
    return pages

def pages_to_fulltext(pages: List[Tuple[int, str]]) -> str:
    buf = []
    for pno, txt in pages:
        buf.append(f"[Page {pno}]\n{txt}")
    return "\n\n".join(buf)

def chunk_text(text: str, max_chars: int=MAX_CHARS_PER_CHUNK) -> List[Tuple[str, str]]:
    """
    [("chunk1-3", "...text..."), ...] の形式で返す
    """
    parts = text.split("\n\n[Page ")
    reconstructed = []
    if parts:
        if parts[0].startswith("[Page "):
            reconstructed.append(parts[0])
        else:
            reconstructed.append(parts[0])  # 先頭に既に [Page ...] がある想定も許容
        for tail in parts[1:]:
            reconstructed.append("[Page " + tail)
    else:
        reconstructed = [text]
    chunks, cur, cur_len, start = [], [], 0, 1
    for i, seg in enumerate(reconstructed, start=1):
        if cur_len + len(seg) > max_chars and cur:
            chunks.append((f"chunk{start}-{i-1}", "\n\n".join(cur).strip()))
            cur, cur_len, start = [seg], len(seg), i
        else:
            cur.append(seg)
            cur_len += len(seg)
    if cur:
        chunks.append((f"chunk{start}-{start+len(cur)-1}", "\n\n".join(cur).strip()))
    return chunks

def ensure_outdir(d: str):
    Path(d).mkdir(parents=True, exist_ok=True)

def list_target_pdfs(root_dir: str) -> List[str]:
    """
    SET_B_DIR 配下のPDFから、パスのどこかに 'rs' を含むもの（ファイル名/ディレクトリ名）は除外。
    """
    root = Path(nfc(root_dir))
    pdfs = []
    for p in root.rglob("*.pdf"):
        lowered_parts = [s.casefold() for s in p.parts]
        name_low = p.name.casefold()
        if ("rs" in name_low) or any("rs" in part for part in lowered_parts):
            continue
        pdfs.append(str(p))
    return sorted(pdfs)

def norm_name(s: str) -> str:
    return unicodedata.normalize("NFKC", s).strip().casefold()

def merge_kpi(base: Dict[str, Any], add: Dict[str, Any]) -> Dict[str, Any]:
    out = {"quantitative_kpi": [], "hard_to_quantify_kpi": []}
    # マージ用セット
    seen_q = set(norm_name(x["name"]) for x in base.get("quantitative_kpi", []))
    seen_h = set(norm_name(x["name"]) for x in base.get("hard_to_quantify_kpi", []))
    out["quantitative_kpi"].extend(base.get("quantitative_kpi", []))
    out["hard_to_quantify_kpi"].extend(base.get("hard_to_quantify_kpi", []))
    for x in add.get("quantitative_kpi", []):
        if norm_name(x["name"]) not in seen_q:
            out["quantitative_kpi"].append(x)
            seen_q.add(norm_name(x["name"]))
    for x in add.get("hard_to_quantify_kpi", []):
        if norm_name(x["name"]) not in seen_h:
            out["hard_to_quantify_kpi"].append(x)
            seen_h.add(norm_name(x["name"]))
    return out

# -------------------------
# 1) intermediate → KPI初期分類（A:定量的 / B:困難）
#    Template を使用し、$payload のみ置換
# -------------------------
#KPI_SEED_PROMPT_TPL = Template("""あなたは日本の行政KPI設計の専門家である。以下に与える複数テキスト断片（intermediate）を精読し、それに含まれるすべての要素について、
#KPI候補を二群に分類して、厳格なJSONのみで出力せよ。冗長説明を含めてはならない。

#分類基準：
#- quantitative_kpi: 目標値・単位・算出法・頻度・データソースが明示可能（あるいは標準化可能）なもの
#- hard_to_quantify_kpi: 主に認知・満足・制度運用の質・行動変容など、定量化が困難または代理指標が必要なもの

#出力スキーマ：
"""{
  "quantitative_kpi": [
    {"name": "<KPI名>", "note": "<任意>"}
  ],
  "hard_to_quantify_kpi": [
    {"name": "<KPI名>", "note": "<任意>"}
  ]
}

入力(複数断片、順不同)：
$payload"""
""")

seed_payload = "\n\n---\n".join(intermediate)
seed_prompt = KPI_SEED_PROMPT_TPL.substitute(payload=seed_payload)
seed_json = call_gpt(seed_prompt, temperature=0.0)
#seed_kpi = safe_json_loads(seed_json)"""

seed_kpi=intermediate

# -------------------------
# 2) SET_B_DIR 内の「RSに含まれていない」PDFを徹底的に読み、KPIを更新
#    Template を使用（$seed_json, $chunk_text）
# -------------------------
KPI_UPDATE_PROMPT_TPL = Template("""あなたは日本の行政KPI設計の専門家である。以下の「事前知識（既存KPI）」を前提に、
続くPDF本文テキスト（1つのチャンク）から新規・改良のKPI候補のみを抽出し、
既存KPIと重複する名称や意味のものは除外して、厳格なJSONのみで返せ。

既存KPI:
$seed_json

出力スキーマ：
{
  "quantitative_kpi": [
    {"name": "<KPI名>","note": "<任意>"}
  ],
  "hard_to_quantify_kpi": [
    {"name": "<KPI名>",  "note": "<任意>"}
  ]
}

PDFテキスト（チャンク）:
$chunk_text
""")

target_pdfs = list_target_pdfs(SET_B_DIR)

updated_kpi = seed_kpi
for idx, pdf_path in enumerate(target_pdfs, 1):
    pages = extract_pdf_pages(pdf_path)
    fulltext = pages_to_fulltext(pages)
    chunks = chunk_text(fulltext, MAX_CHARS_PER_CHUNK)
    for cname, ctext in chunks:
        prompt = KPI_UPDATE_PROMPT_TPL.substitute(
            seed_json=json.dumps(updated_kpi, ensure_ascii=False),
            chunk_text=ctext
        )
        try:
            add_json = call_gpt(prompt, temperature=0.0)
            add_kpi = safe_json_loads(add_json)
            updated_kpi = merge_kpi(updated_kpi, add_kpi)
        except Exception as e:
            # 壊れたJSON等はスキップ
            pass
        time.sleep(SLEEP_BETWEEN_CALLS)
        print(updated_kpi)

import os, json, re, time, unicodedata, math
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from string import Template

from duckduckgo_search import DDGS  # 既定検索

# ============================================================
# A. 検索クライアント
#    - 既定: DuckDuckGo（APIキー不要）
#    - 環境変数があれば SerpAPI / Bing / Google CSE に切替可能
# ============================================================

class WebSearch:
    def __init__(self):
        self.mode = None
        if os.environ.get("SERPAPI_API_KEY"):
            self.mode = "serpapi"
        elif os.environ.get("BING_SEARCH_V7_SUBSCRIPTION_KEY"):
            self.mode = "bing"
        elif os.environ.get("GOOGLE_API_KEY") and os.environ.get("GOOGLE_CSE_ID"):
            self.mode = "google_cse"
        else:
            self.mode = "ddg"  # 既定

    def search(self, query: str, max_results: int = 8, lang: str = "ja") -> List[Dict[str, str]]:
        if self.mode == "ddg":
            return self._ddg_search(query, max_results, lang)
        elif self.mode == "serpapi":
            return self._serpapi_search(query, max_results)
        elif self.mode == "bing":
            return self._bing_search(query, max_results)
        elif self.mode == "google_cse":
            return self._google_cse_search(query, max_results)
        else:
            return self._ddg_search(query, max_results, lang)

    def _ddg_search(self, query, max_results, lang):
        items = []
        with DDGS() as ddgs:
            for r in ddgs.text(query, region="jp-jp", safesearch="moderate", max_results=max_results, timelimit=None):
                # r: {'title','href','body'}
                items.append({
                    "title": r.get("title", "")[:300],
                    "url": r.get("href", ""),
                    "snippet": r.get("body", "")[:500]
                })
        return items

    def _serpapi_search(self, query, max_results):
        # 使う場合は pip install google-search-results を導入し、ここを実装
        # from serpapi import GoogleSearch
        # ...
        return []

    def _bing_search(self, query, max_results):
        # 使う場合は Azure Bing Web Search v7 のエンドポイントに requests で
        # ...
        return []

    def _google_cse_search(self, query, max_results):
        # 使う場合は Google Programmable Search (CSE) の REST に requests で
        # ...
        return []

search_client = WebSearch()

# ============================================================
# B. Webコンテキストの構築
#    - KPI名から自動で複数クエリを生成（統計・要因・効果量・因果推定・費用効果）
#    - 検索要約（タイトル/スニペット/URL）をモデルに渡す
#    - トークン節約のため文字長カット
# ============================================================

def build_queries_for_kpi(kpi_name: str) -> List[str]:
    base = unicodedata.normalize("NFKC", kpi_name)
    qs = [
        f"{base} 統計 データ 政府",
        f"{base} 指標 定義 単位 測定方法",
        f"{base} 要因 先行研究 系統的レビュー",
        f"{base} 政策 効果 弾性 影響 回帰",
        f"{base} 介入 効果量 メタアナリシス",
        f"{base} 費用効果 便益費用 CBA",
        f"{base} e-Stat 統計",
    ]
    # 冗長回避（ユニーク化）
    out, seen = [], set()
    for q in qs:
        if q not in seen:
            out.append(q); seen.add(q)
    return out

def fetch_web_context_for_kpi(kpi_name: str, per_query: int = 4, max_total: int = 20) -> str:
    queries = build_queries_for_kpi(kpi_name)
    rows = []
    for q in queries:
        results = search_client.search(q, max_results=per_query, lang="ja")
        for r in results:
            rows.append(f"- title: {r['title']}\n  url: {r['url']}\n  snippet: {r['snippet']}")
            if len(rows) >= max_total:
                break
        if len(rows) >= max_total:
            break
    ctx = "WEB_RESULTS:\n" + "\n".join(rows)
    # 安全のため最大長を制限
    return ctx[:24000]

# ============================================================
# C. 改良 PATHWAY プロンプト
#    - 目的：予算→…→KPI上昇 のDAG（任意サイズ）をJSONで生成
#    - ノード：定量可能＋過去データ参照可能（データ候補URL）
#    - エッジ：符号・遅延・重み（弾性/効果量）・信頼度・根拠（PDF & Web）
#    - 厳格にJSONのみ出力
# ============================================================

PATHWAY_PROMPT_TPL = Template(r"""あなたは政策因果ネットワーク設計者である。
以下の材料（PDFコーパス全文の要約断片＋Web検索結果）を用い、指定KPIについて、
「政策にかける予算」から当該KPIの上昇までの**有向非巡回グラフ（DAG）**を提案せよ。
出力は**厳格にJSONのみ**とし、説明文は一切含めない。

要件（厳守）：
1) 開始ノードは必ず `{"id":"BUDGET","label":"政策にかける予算","type":"budget","unit":"円"}`。
2) 終端ノードは必ず `{"id":"KPI_DELTA","label":"${kpi_name}の上昇","type":"kpi"}`。
3) 中間ノード数は任意だが、10以上であることを推奨し、複雑なグラフ構造であることが望ましい。
4) 指定KPIが "quantitative_kpi" に含まれる場合：中間ノードはすべて**定量可能**かつ**過去データ（時系列等）が入手可能**であること。 計測困難な概念は代理指標を必ず `indicator_spec` に記す。
   指定KPIが "hard_to_quantify_kpi" に含まれる場合：中間ノードについて定量性の要件は緩和し、代理指標や概念的ノードを許容する。
5) 各ノードには `indicator_spec` を含める：{definition, unit, measurement, historical_series_candidates:[URL...]}。可能なら baseline 候補値とソース。
6) 各エッジには**符号 sign（+/-）**、**遅延 expected_lag_months**、**重み weight**（効果量; 可能なら**弾性/半弾性/限界効果**のいずれかを `weight_type` で明示）、95%信頼区間 `weight_ci`（分からなければ null）、根拠 `evidence`（pdf: page & quote / web: url & title & snippet）、根拠の信頼度 `confidence`（0-1）。
7) ノード・エッジには**重複/同義**の正規化を行い、名詞句で表現。
8) ロジックモデル（Activity→Output→Outcomes）は**参考**にとどめ、新たな有向グラフを自由に構成してよい。
9) 重みが資料から厳密に取得できない場合は、`weight=null` とし、`assumptions` に**推定のための事前分布（例：半弾性 ~ N(0, 0.1)）**を明記。
11) 出力は以下のスキーマに**完全準拠**：

{
  "kpi": "${kpi_name}",
  "graph": {
    "nodes": [
      {
        "id": "BUDGET",
        "label": "政策にかける予算",
        "type": "budget",
        "unit": "円",
        "indicator_spec": {
          "definition": "政策に投入する金額（当該プログラムの年間支出）",
          "unit": "円",
          "measurement": "当該予算書・決算ベース",
          "historical_series_candidates": ["https://www.mof.go.jp/","https://www.soumu.go.jp/"]
        },
        "baseline_candidates": [],
        "typical_range": null
      }
      // 中間ノード（idは N1, N2, ... など）、終端ノード KPI_DELTA を含める
    ],
    "edges": [
      {
        "from": "BUDGET",
        "to": "N1",
        "sign": "+",
        "expected_lag_months": 6,
        "weight_type": "elasticity | semi_elasticity | marginal_effect | conversion_rate",
        "weight": null,
        "weight_ci": null,
        "evidence": {
          "pdf": [{"page":"pX","quote":"..."}],
          "web": [{"url":"https://...","title":"...","snippet":"..."}]
        },
        "confidence": 0.0
      }
      // 必要なだけ列挙。最終的に KPI_DELTA へ到達するパスを必ず構成
    ]
  },
  "path_decomposition": [
    {"path": ["BUDGET","...","KPI_DELTA"], "rationale": "<要旨>", "dominant_links": ["..."]}
  ],
  "calculation_stub": {
    "assumptions": [
      "重みが弾性なら log(KPI) 変化率 = Σ( path weights × 入力変化率 )",
      "重みが限界効果なら ΔKPI = Σ( path weights × 入力のΔ )"
    ],
    "pseudocode": [
      "1) 予算Δを各下流ノードに伝播（エッジの sign / weight / lag を考慮）",
      "2) 各ノードのΔを集約し、KPI_DELTAに到達した合成効果を算出",
      "3) 不明重みは事前分布でサンプリング→感度分析"
    ]
  },
  "sources_used": {
    "pdf_pages_cited": ["pX","pY"],
    "web_urls": ["https://...", "..."]
  },
  "assumptions": [
    "省庁統計の定義に準拠して指標を測定",
    "因果識別の質は evidence.confidence に反映（RCT>準実験>観察>専門推定）"
  ],
  "flags": {
    "acyclic": true,
    "needs_more_evidence": false
  }
}

材料（PDFとWebの要約をそのまま貼る）：
【PDF要約断片（結合）】
$document_context

【検索結果（タイトル・スニペ・URL）】
$web_context
""")

# ============================================================
# D. KPIごとにネットワーク生成（Web検索併用）
#    - 既存: updated_kpi（quantitative_kpi / hard_to_quantify_kpi）
#    - 既存: target_pdfs, extract_pdf_pages, pages_to_fulltext など
# ============================================================

def build_pdf_context_from_paths(pdf_paths: List[str], max_chars: int = 120000) -> str:
    parts, total = [], 0
    for path in pdf_paths:
        pages = extract_pdf_pages(path)
        txt = pages_to_fulltext(pages)
        block = f"=== FILE: {path} ===\n{txt}\n"
        if total + len(block) > max_chars:
            # 切り詰め（末尾優先か先頭優先かは適宜調整）
            remain = max_chars - total
            parts.append(block[:max(0, remain)])
            break
        parts.append(block); total += len(block)
    return "\n".join(parts)

# 既存の target_pdfs を再利用（RS1を含まないPDF群）
document_context = build_pdf_context_from_paths(target_pdfs, max_chars=120000)

def uniq_list(seq):
    out, seen = [], set()
    for x in seq:
        if x not in seen:
            out.append(x); seen.add(x)
    return out

quant_names = [k["name"] for k in updated_kpi.get("quantitative_kpi", [])]
hard_names  = [k["name"] for k in updated_kpi.get("hard_to_quantify_kpi", [])]
all_kpi_names = uniq_list(quant_names + hard_names)

per_kpi_models = []
for i, kname in enumerate(all_kpi_names, 1):
    # KPIオブジェクトのメタを付与（任意で）
    k_obj = {"name": kname}
    for k in updated_kpi.get("quantitative_kpi", []):
        if norm_name(k["name"]) == norm_name(kname):
            k_obj.update(k)
    for k in updated_kpi.get("hard_to_quantify_kpi", []):
        if norm_name(k["name"]) == norm_name(kname):
            k_obj.update(k)

    # Webコンテキストを収集
    web_ctx = fetch_web_context_for_kpi(kname, per_query=4, max_total=20)

    # プロンプトを生成
    prompt = PATHWAY_PROMPT_TPL.substitute(
        kpi_name=kname,
        document_context=document_context,
        web_context=web_ctx
    )

    try:
        out_text = call_gpt(prompt, temperature=0.0)
        out_json = json.loads(out_text)  # まず素直に
    except Exception:
        # 壊れた時はセーフパーサで復旧
        out_json = safe_json_loads(out_text)

    # 必須ノードの存在/終端到達チェックの軽い検証（失敗時はフラグ）
    def has_node(nodes, nid):
        return any(n.get("id") == nid for n in nodes)
    def reaches_kpi(edges):
        # KPI_DELTA を末端に持つ辺があるかの粗チェック（厳密なDAG到達判定は任意）
        return any(e.get("to") == "KPI_DELTA" for e in edges)

    graph = out_json.get("graph", {})
    nodes = graph.get("nodes", [])
    edges = graph.get("edges", [])

    flags = out_json.setdefault("flags", {})
    if not has_node(nodes, "BUDGET"):
        flags["needs_more_evidence"] = True
    if not any(n.get("id") == "KPI_DELTA" for n in nodes):
        flags["needs_more_evidence"] = True
    if not reaches_kpi(edges):
        flags["needs_more_evidence"] = True

    per_kpi_models.append(out_json)
    print(f"[{i}/{len(all_kpi_names)}] built graph for KPI: {kname}")


# -------------------------
# 最終JSONの構築と保存（OUT_DIR/RS_batch_json.json）
# -------------------------
ensure_outdir(OUT_DIR)
final_payload = {
    "kpi_catalog": updated_kpi,    # 1+2の結果（定量可能KPI / 困難KPI）
    "per_kpi_effect_models": per_kpi_models,  # 3の結果（KPIごとの効果発現経路）
    "meta": {
        "set_b_dir": SET_B_DIR,
        "excluded_rule": "パス（ファイル名/ディレクトリ名）に 'RS1' を含むPDFを除外",
        "model": MODEL,
        "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
    }
}
out_path = str(Path(OUT_DIR) / OUT_JSON)
with open(out_path, "w", encoding="utf-8") as f:
    json.dump(final_payload, f, ensure_ascii=False, indent=2)

print(f"[DONE] 出力: {out_path}")
print("  - KPI（定量可能/困難）の件数:",
      len(updated_kpi.get("quantitative_kpi", [])),
      len(updated_kpi.get("hard_to_quantify_kpi", [])))
print("  - KPIごとの効果発現経路:", len(per_kpi_models))



# ============================================================
# 追加セル：効果発現経路の各ノードについて
#           「過去10点のデータ」をオンライン検索し、JSONに格納
# 依存：requests, bs4, trafilatura  ※外部APIキー不要（DuckDuckGoをHTMLスクレイピング）
# 前提：前セルまでで client = OpenAI(), MODEL, OUT_DIR, OUT_DIR/RS_batch_json.json が存在
# ============================================================

import os, re, json, time, unicodedata
from pathlib import Path
from urllib.parse import quote_plus, urlparse
import requests
from bs4 import BeautifulSoup
import trafilatura
from string import Template

# ----------- 設定 -----------
DDG_BASE = "https://duckduckgo.com/html/?q="
USER_AGENT = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36"
MAX_RESULTS_PER_QUERY = 6
MAX_PAGES_PER_NODE    = 12
FETCH_TIMEOUT         = 15
PAUSE_SEC             = 1.0
TS_OUT_PATH           = str(Path(OUT_DIR) / "per_node_timeseries.json")

PREFERRED_DOMAINS = [
    "go.jp", "e-stat.go.jp", "mhlw.go.jp", "cao.go.jp", "meti.go.jp", "mext.go.jp",
    "soumu.go.jp", "stat.go.jp", "e-gov.go.jp", "data.go.jp", "who.int", "oecd.org",
    "worldbank.org", "ilo.org", "imf.org", "un.org", "gov.uk", "europa.eu"
]

session = requests.Session()
session.headers.update({"User-Agent": USER_AGENT})

# ============================================================
# 1) 効果発現経路からノード集合を抽出
# ============================================================
batch_path = str(Path(OUT_DIR) / "RS_batch_json.json")
with open(batch_path, "r", encoding="utf-8") as f:
    batch_data = json.load(f)

def collect_node_labels(per_kpi_models):
    labels = set()
    for item in per_kpi_models:
        ep = item.get("effect_pathway") or {}
        for cat in ("activities", "outputs"):
            for n in ep.get(cat, []) or []:
                lab = (n.get("label") or "").strip()
                if lab:
                    labels.add(lab)
        outs = ep.get("outcomes") or {}
        for tier in ("short", "mid", "long"):
            for n in outs.get(tier, []) or []:
                lab = (n.get("label") or "").strip()
                if lab:
                    labels.add(lab)
    return sorted(labels)

per_kpi_models = batch_data.get("per_kpi_effect_models", [])
node_labels = collect_node_labels(per_kpi_models)
print(f"[info] ノード候補数: {len(node_labels)}")

# ============================================================
# 2) 検索クエリの生成（モデル）… Template（$label）で安全に
# ============================================================
QUERY_PROMPT_TPL = Template("""
あなたは公共データの探索者である。与えられた政策ノード名（名詞句）について、
その概念を**過去データ（時系列）**として探すのに適したウェブ検索クエリを日本語と英語で設計せよ。
計測対象（何の数値か）、単位、期間（年度・月）、日本の公的統計や国際機関の代表的ソースも想定すること。
**必ず厳格なJSONのみ**を返す。

スキーマ：
{
  "queries": ["<q1>", "<q2>", "..."],
  "hints": {
    "unit_candidates": ["%","人","件","時間","指数","円"],
    "source_hints": ["e-Stat","厚労省","総務省","経産省","OECD","世界銀行"],
    "notes": "<何を測る想定かを1〜2文>"
  }
}

ノード:
$label
""")

def propose_queries_for_label(label: str) -> dict:
    prompt = QUERY_PROMPT_TPL.substitute(label=label)
    resp = client.responses.create(model=MODEL, input=prompt, temperature=0.0)
    text = resp.output_text.strip()
    try:
        return json.loads(text)
    except Exception:
        m1, m2 = text.find("{"), text.rfind("}")
        if m1 != -1 and m2 != -1 and m2 > m1:
            return json.loads(re.sub(r",\s*([}\]])", r"\1", text[m1:m2+1]))
        return {"queries":[label], "hints":{"unit_candidates":[],"source_hints":[],"notes":""}}

# ============================================================
# 3) DuckDuckGo検索 & 本文抽出
# ============================================================
def is_preferred(url: str) -> bool:
    h = urlparse(url).netloc.lower()
    return any(dom in h for dom in PREFERRED_DOMAINS)

def ddg_search(q: str, max_results=MAX_RESULTS_PER_QUERY):
    url = DDG_BASE + quote_plus(q)
    r = session.get(url, timeout=FETCH_TIMEOUT)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "html5lib")
    out = []
    for a in soup.select("a.result__a"):
        href = a.get("href")
        title = a.get_text(" ", strip=True)
        if not href:
            continue
        out.append({"title": title, "url": href})
        if len(out) >= max_results:
            break
    # 重複排除 + 公的サイト優先
    seen, uniq = set(), []
    for it in out:
        u = it["url"]
        if u in seen:
            continue
        seen.add(u)
        uniq.append(it)
    uniq.sort(key=lambda x: (not is_preferred(x["url"])))
    return uniq

def fetch_and_extract(url: str) -> dict:
    try:
        r = session.get(url, timeout=FETCH_TIMEOUT)
        r.raise_for_status()
        downloaded = trafilatura.extract(
            r.text, include_comments=False, include_tables=True,
            no_fallback=False, url=url
        )
        text = downloaded or ""
        # タイトル（失敗しても無視）
        title = ""
        try:
            soup = BeautifulSoup(r.text, "html5lib")
            t = soup.find("title")
            if t:
                title = t.get_text(" ", strip=True)
        except Exception:
            pass
        return {"url": url, "title": title, "text": text[:200000]}
    except Exception:
        return {"url": url, "title": "", "text": ""}

def chunk_corpus_text(pages, max_chars=120000):
    texts = []
    for p in pages:
        if not p["text"]:
            continue
        texts.append(f"### {p['title']}\n{p['text']}\n")
    merged = "\n\n".join(texts)
    srcs = "\n".join([f"- {p['title']} [SRC] {p['url']}" for p in pages])
    merged = (merged[:max_chars] + "\n\n[SOURCES]\n" + srcs) if len(merged) > max_chars else (merged + "\n\n[SOURCES]\n" + srcs)
    return merged

# ============================================================
# 4) 本文→時系列抽出（Templateで安全に）
# ============================================================
EXTRACT_TS_PROMPT_TPL = Template("""
あなたは統計リーダーである。以下に複数のウェブページ本文テキストを与える。
目標は、与えられた「指標候補（ノード名）」に関係しうる**時系列の定量値**を最大10点、**新しい順**に抽出すること。
日付（YYYY-MM-DD 可能なら）、値（数値）、単位、指標名、出典URL、出典タイトルを付け、**厳格なJSONのみ**を返せ。

注意：
- 本文に数値が複数の指標で出る場合は、与えられたノードに最も近い指標を優先。
- 年だけの記述は "YYYY-01-01" として記録可。月のみは "YYYY-MM-01" とする。
- 単位や指数の基準年があれば "unit_note" に記載。
- 出典が曖昧なら "confidence" を下げる。
- 10点に満たない場合は取得できた範囲でよい（0でも可）。
- JSON以外の文字を一切出力しない。

スキーマ：
{
  "label": "<ノード名>",
  "timeseries": [
    {
      "date": "YYYY-MM-DD",
      "value": 0.0,
      "unit": "<単位>",
      "measure_name": "<指標名>",
      "source_url": "<URL>",
      "source_title": "<タイトル>",
      "unit_note": "<任意>",
      "confidence": 0.0
    }
  ]
}

ノード名:
$label

本文コーパス（検索で得た複数ページの要約本文。URLは本文末尾の [SRC] に付与）:
$corpus
""")

def process_one_label(label: str) -> dict:
    # a) クエリ生成
    qobj = propose_queries_for_label(label)
    queries = qobj.get("queries") or [label]

    # b) 検索 & 本文抽出
    collected = []
    for q in queries:
        hits = ddg_search(q, max_results=MAX_RESULTS_PER_QUERY)
        for h in hits:
            if len(collected) >= MAX_PAGES_PER_NODE:
                break
            page = fetch_and_extract(h["url"])
            if len((page.get("text") or "")) >= 400:
                collected.append(page)
            time.sleep(PAUSE_SEC)
        if len(collected) >= MAX_PAGES_PER_NODE:
            break

    corpus = chunk_corpus_text(collected)

    # c) 抽出（Templateで置換。str.formatは使わない）
    prompt = EXTRACT_TS_PROMPT_TPL.substitute(label=label, corpus=corpus)
    resp = client.responses.create(model=MODEL, input=prompt, temperature=0.0)
    text = resp.output_text.strip()
    try:
        js = json.loads(text)
    except Exception:
        m1, m2 = text.find("{"), text.rfind("}")
        if m1 != -1 and m2 != -1 and m2 > m1:
            js = json.loads(re.sub(r",\s*([}\]])", r"\1", text[m1:m2+1]))
        else:
            js = {"label": label, "timeseries": []}

    # 新しい順に最大10点へ整列
    def date_key(d): return d.get("date") or ""
    js["timeseries"] = sorted(js.get("timeseries", []), key=date_key, reverse=True)[:10]
    return js

# ============================================================
# 5) 全ノード実行 & 保存
# ============================================================
"""per_node_timeseries = {}
for i, label in enumerate(node_labels, 1):
    print(f"[{i}/{len(node_labels)}] {label} ...")
    try:
        ts = process_one_label(label)
        per_node_timeseries[label] = ts["timeseries"]
    except Exception as e:
        per_node_timeseries[label] = []
        print("   -> error:", type(e).__name__, e)
    time.sleep(PAUSE_SEC)

with open(TS_OUT_PATH, "w", encoding="utf-8") as f:
    json.dump(per_node_timeseries, f, ensure_ascii=False, indent=2)

print(f"[DONE] 保存: {TS_OUT_PATH}")
print(f"  収集ノード数: {len(per_node_timeseries)}")"""


from itertools import islice

# ============================================================
# 5) 最初の11ノードのみ実行 & 保存
# ============================================================
per_node_timeseries = {}
for i, label in enumerate(islice(node_labels, 11), 1):  # ← ここで11件に制限
    print(f"[{i}/{min(11, len(node_labels))}] {label} ...")
    try:
        ts = process_one_label(label)
        per_node_timeseries[label] = ts["timeseries"]
    except Exception as e:
        per_node_timeseries[label] = []
        print("   -> error:", type(e).__name__, e)
    time.sleep(PAUSE_SEC)

with open(TS_OUT_PATH, "w", encoding="utf-8") as f:
    json.dump(per_node_timeseries, f, ensure_ascii=False, indent=2)

print(f"[DONE] 保存: {TS_OUT_PATH}")
print(f"  収集ノード数: {len(per_node_timeseries)}")

# ============================================================
# 追加セル：Transfer Entropy（X→Y, Y→X）による因果検出
#   - 離散化（分位点ビニング）
#   - TE = I(Y_t ; X_{t-1} | Y_{t-1}) をラグ1で推定
#   - 置換テストでp値推定（小標本の頑健化）
# 前提：
#   - OUT_DIR, RS_batch_json.json, per_node_timeseries.json が存在
#   - pandas, numpy, scipy を使用
# ============================================================


import json
import numpy as np
import pandas as pd
from pathlib import Path
from dateutil import parser
from math import log
from scipy.stats import rankdata

BATCH_PATH = str(Path(OUT_DIR) / "RS_batch_json.json")
TS_PATH    = str(Path(OUT_DIR) / "per_node_timeseries.json")
TE_JSON_OUT = str(Path(OUT_DIR) / "transfer_entropy_results.json")
TE_CSV_OUT  = str(Path(OUT_DIR) / "transfer_entropy_results.csv")

# ---------------------------
# 1) 時系列の整形ユーティリティ
# ---------------------------
def parse_date(s):
    try:
        return pd.to_datetime(parser.parse(s).date())
    except Exception:
        return pd.NaT

def series_from_timeseries_items(items):
    """ items: [{date, value, ...}] -> pandas.Series(index=date, values=float) """
    if not items:
        return pd.Series(dtype="float64")
    df = pd.DataFrame(items)
    if "date" not in df or "value" not in df:
        return pd.Series(dtype="float64")
    df["date"] = df["date"].apply(parse_date)
    df = df.dropna(subset=["date"]).copy()
    # 同日重複があれば平均
    df = df.groupby("date", as_index=False)["value"].mean().sort_values("date")
    s = pd.Series(df["value"].values, index=df["date"].values).sort_index()
    return s

def align_common_points(sx, sy, require_len=10):
    """同一日付の共通点を抽出。十分あれば**直近の10点**を返す。"""
    idx = sx.index.intersection(sy.index)
    if len(idx) < require_len:
        return pd.Series(dtype="float64"), pd.Series(dtype="float64")
    idx = idx.sort_values()[-require_len:]  # 新しい順に10点確保
    return sx.loc[idx], sy.loc[idx]

# ---------------------------
# 2) 分位点ビニング（離散化）
# ---------------------------
def quantile_bins(x, n_bins=3):
    """
    分位点で離散化（3〜5程度が小標本向き）。
    重複値対策として rankdata → 分位点を計算。
    戻り値: 離散ラベル配列（0..n_bins-1）
    """
    x = np.asarray(x, float)
    ranks = rankdata(x, method="average") / (len(x) + 1.0)  # (0,1)近傍
    qs = np.linspace(0, 1, n_bins + 1)
    # 右端を含まない区間、最後のみ含む
    z = np.digitize(ranks, qs[1:-1], right=False)
    z[z == n_bins] = n_bins - 1
    return z

# ---------------------------
# 3) 条件付き相互情報 I(Y; X | Z) の推定（離散）
#    I = H(Y,Z)+H(X,Z)-H(X,Y,Z)-H(Z)
#    Hは多次元離散エントロピー（ラプラススムージング）
# ---------------------------
def entropy_discrete(vars_tuple, alpha=1.0):
    """
    vars_tuple: 形状 (m, T) の int 配列（m次元離散変数, Tサンプル）
    ラプラススムージング alpha を使用
    """
    # 多次元カテゴリを線形化して頻度
    X = np.vstack(vars_tuple).astype(int)
    base = int(np.max(X)) + 1  # 粗い上限（全変数で同一bin数前提ならOK）
    # ただし各変数の最大値が異なっても問題ないように、ユニークタプルで計算
    keys, counts = np.unique(X.T, axis=0, return_counts=True)
    n = X.shape[1]
    # 有効カテゴリ数 K は keys のユニーク数（理論上は ∏bin_i だがスパース）
    K = keys.shape[0]
    probs = (counts + alpha) / (n + alpha * K)
    return -np.sum(probs * np.log(probs + 1e-15))

def conditional_mutual_information(Y, X, Z, alpha=1.0):
    """
    Y, X, Z: 離散ラベル配列（同じ長さ）
    I(Y;X|Z) = H(Y,Z) + H(X,Z) - H(X,Y,Z) - H(Z)
    """
    Y = np.asarray(Y, int)
    X = np.asarray(X, int)
    Z = np.asarray(Z, int)
    assert len(Y) == len(X) == len(Z)
    H_YZ  = entropy_discrete((Y, Z), alpha=alpha)
    H_XZ  = entropy_discrete((X, Z), alpha=alpha)
    H_XYZ = entropy_discrete((X, Y, Z), alpha=alpha)
    H_Z   = entropy_discrete((Z,), alpha=alpha)
    return H_YZ + H_XZ - H_XYZ - H_Z

# ---------------------------
# 4) Transfer Entropy 推定（ラグ1、置換検定）
#    TE_{X→Y} = I(Y_t ; X_{t-1} | Y_{t-1})
# ---------------------------
def transfer_entropy_xy(x, y, n_bins=3, n_perm=1000, alpha=1.0, random_state=0):
    """
    x, y: 同期済み実数列（長さT）
    1ステップ遅れのTEを離散化ベースで推定
    置換検定: X_{t-1} をシャッフルして帰無分布を近似
    戻り値: {"te": float, "pvalue": float, "n": int, "bins": int}
    """
    rng = np.random.default_rng(random_state)
    x = np.asarray(x, float)
    y = np.asarray(y, float)
    assert len(x) == len(y)
    T = len(x)
    if T < 3:
        return {"te": np.nan, "pvalue": np.nan, "n": T, "bins": n_bins}

    # ラグ構築（t=1..T-1が有効）
    Xlag = x[:-1]
    Ylag = y[:-1]
    Ynow = y[1:]
    n = len(Ynow)

    # 離散化（分位点ビン）
    Xd = quantile_bins(Xlag, n_bins)
    Yd = quantile_bins(Ynow, n_bins)
    Zd = quantile_bins(Ylag, n_bins)

    te_hat = conditional_mutual_information(Yd, Xd, Zd, alpha=alpha)

    # 置換検定：X_{t-1} をシャッフル
    null_vals = np.empty(n_perm)
    for i in range(n_perm):
        Xperm = rng.permutation(Xd)
        null_vals[i] = conditional_mutual_information(Yd, Xperm, Zd, alpha=alpha)

    # 右片側p値（TEが大きいほど因果強）
    p = (np.sum(null_vals >= te_hat) + 1.0) / (n_perm + 1.0)
    return {"te": float(te_hat), "pvalue": float(p), "n": int(n), "bins": int(n_bins)}

# ---------------------------
# 5) データ読み込み & エッジごとに TE(X→Y), TE(Y→X)
# ---------------------------
with open(BATCH_PATH, "r", encoding="utf-8") as f:
    batch = json.load(f)
with open(TS_PATH, "r", encoding="utf-8") as f:
    node_ts = json.load(f)

# ラベル → Series へ
label2series = {lab: series_from_timeseries_items(ts) for lab, ts in node_ts.items()}

# エッジ一覧
edges = []
for item in batch.get("per_kpi_effect_models", []):
    ep = item.get("effect_pathway") or {}
    for e in ep.get("edges", []) or []:
        src, dst = e.get("from"), e.get("to")
        if src and dst:
            edges.append((src, dst))
# 重複除去（順序保持）
edges = list(dict.fromkeys(edges))

results = []
N_BINS = 3          # 小標本向けに3ビンを既定（必要なら4〜5に調整）
N_PERM = 2000       # 置換検定の反復数（計算資源に合わせて調整）
ALPHA  = 1.0        # ラプラススムージング

for (src, dst) in edges:
    sx = label2series.get(src, pd.Series(dtype="float64"))
    sy = label2series.get(dst, pd.Series(dtype="float64"))
    # 共通10点の整合
    ax, ay = align_common_points(sx, sy, require_len=10)
    if len(ax) != 5 or len(ay) != 5:
        results.append({
            "edge": {"from": src, "to": dst},
            "status": "skip_not_enough_common_points",
            "common_points": int(min(len(ax), len(ay)))
        })
        continue

    # 実数配列へ
    x = ax.values.astype(float)
    y = ay.values.astype(float)

    # X→Y, Y→X の双方向TE
    te_xy = transfer_entropy_xy(x, y, n_bins=N_BINS, n_perm=N_PERM, alpha=ALPHA, random_state=0)
    te_yx = transfer_entropy_xy(y, x, n_bins=N_BINS, n_perm=N_PERM, alpha=ALPHA, random_state=1)

    results.append({
        "edge": {"from": src, "to": dst},
        "status": "ok",
        "common_points": 10,
        "te_x_to_y": te_xy,
        "te_y_to_x": te_yx,
        "notes": {
            "definition": "TE_{X→Y} = I(Y_t; X_{t-1} | Y_{t-1}) with discrete quantile binning",
            "lag": 1,
            "bins": N_BINS,
            "permutations": N_PERM,
            "smoothing_alpha": ALPHA
        }
    })

# 保存（JSON & CSV）
with open(TE_JSON_OUT, "w", encoding="utf-8") as f:
    json.dump({"results": results}, f, ensure_ascii=False, indent=2)

# CSVは主要列のみ
rows = []
for r in results:
    if r["status"] != "ok":
        rows.append({
            "from": r["edge"]["from"],
            "to": r["edge"]["to"],
            "status": r["status"],
            "common_points": r.get("common_points", 0),
            "te_x_to_y": np.nan, "p_x_to_y": np.nan,
            "te_y_to_x": np.nan, "p_y_to_x": np.nan
        })
    else:
        rows.append({
            "from": r["edge"]["from"],
            "to": r["edge"]["to"],
            "status": r["status"],
            "common_points": r["common_points"],
            "te_x_to_y": r["te_x_to_y"]["te"],
            "p_x_to_y": r["te_x_to_y"]["pvalue"],
            "te_y_to_x": r["te_y_to_x"]["te"],
            "p_y_to_x": r["te_y_to_x"]["pvalue"]
        })
pd.DataFrame(rows).to_csv(TE_CSV_OUT, index=False)

print(f"[DONE] Transfer Entropy 結果: {TE_JSON_OUT} / {TE_CSV_OUT}")
print("  実行済み:", sum(1 for r in results if r["status"] == "ok"))
print("  スキップ:", sum(1 for r in results if r["status"].startswith("skip")))

# =========================================
# 追加セル：10点条件のVAR-グレンジャー因果テスト
# 前提：OUT_DIR, RS_batch_json.json, per_node_timeseries.json が存在
# =========================================

import json, math, warnings
import numpy as np
import pandas as pd
from pathlib import Path
from dateutil import parser
from statsmodels.tsa.api import VAR
from statsmodels.tsa.stattools import adfuller

warnings.filterwarnings("ignore")

BATCH_PATH = str(Path(OUT_DIR) / "RS_batch_json.json")
TS_PATH    = str(Path(OUT_DIR) / "per_node_timeseries.json")
OUT_PATH   = str(Path(OUT_DIR) / "granger_results.json")

# ---- ユーティリティ ----
def parse_date(s):
    try:
        return pd.to_datetime(parser.parse(s).date())
    except Exception:
        return pd.NaT

def build_series(ts_list):
    """ per_node_timeseries.json の1ノード [{date, value, unit, ...}] → Series(date→value) """
    df = pd.DataFrame(ts_list)
    if df.empty:
        return pd.Series(dtype="float64")
    df["date"] = df["date"].apply(parse_date)
    df = df.dropna(subset=["date"])
    # 同一日付が複数あれば平均（他の集計でも可）
    df = df.groupby("date", as_index=False)["value"].mean().sort_values("date")
    s = pd.Series(df["value"].values, index=df["date"].values).sort_index()
    return s

def align_two(sx, sy, require_len=10):
    """ 同一日付に内積、ちょうどor以上 10 点を確保 """
    idx = sx.index.intersection(sy.index)
    sx2, sy2 = sx.loc[idx], sy.loc[idx]
    # 10点以上あれば直近10点に切り詰め（新しい順に10点）
    if len(idx) >= require_len:
        take = idx.sort_values()[-require_len:]  # 新しい10点
        return sx2.loc[take], sy2.loc[take]
    return pd.Series(dtype="float64"), pd.Series(dtype="float64")

def make_stationary(x):
    """
    対数差分可能なら log→diff1、非正なら diff1。
    返り値: 変換後Series, method(str)
    """
    x = x.astype(float)
    eps = 1e-9
    if (x > 0).all():
        y = np.log(x + eps).diff().dropna()
        return y, "logdiff1"
    else:
        y = x.diff().dropna()
        return y, "diff1"

def adf_ok(x):
    try:
        res = adfuller(x, autolag="AIC")
        return bool(res[1] < 0.1)  # ゆるめ閾値、サンプル10ではほぼ無力だが参考
    except Exception:
        return False

def zscore(df):
    return (df - df.mean()) / (df.std(ddof=0) + 1e-12)

# ---- データ読み込み ----
with open(BATCH_PATH, "r", encoding="utf-8") as f:
    batch = json.load(f)
with open(TS_PATH, "r", encoding="utf-8") as f:
    per_node_ts = json.load(f)

# ラベル→Series へ
label2series = {lab: build_series(ts) for lab, ts in per_node_ts.items()}

# 対象エッジ抽出
edges = []
for item in batch.get("per_kpi_effect_models", []):
    ep = item.get("effect_pathway") or {}
    for e in ep.get("edges", []):
        src, dst = e.get("from"), e.get("to")
        if src and dst:
            edges.append((src, dst))
# 重複除去
edges = list(dict.fromkeys(edges))

results = []
for (src, dst) in edges:
    sx = label2series.get(src, pd.Series(dtype="float64"))
    sy = label2series.get(dst, pd.Series(dtype="float64"))

    # 10点の共通日付を確保
    ax, ay = align_two(sx, sy, require_len=10)
    if len(ax) == 0 or len(ay) == 0 or len(ax) != 10:
        results.append({
            "edge": {"from": src, "to": dst},
            "status": "skip_not_enough_common_points",
            "common_points": int(min(len(ax), len(ay)))
        })
        continue

    # 定常化（ログ差分 or 一次差分）
    x_tr, m1 = make_stationary(ax)
    y_tr, m2 = make_stationary(ay)
    # 差分で1点減るので、両者の共通indexで再アライン
    idx2 = x_tr.index.intersection(y_tr.index)
    x_tr, y_tr = x_tr.loc[idx2], y_tr.loc[idx2]

    if len(idx2) < 8:  # VAR(1) 推定の最低ラインを安全側に
        results.append({
            "edge": {"from": src, "to": dst},
            "status": "skip_after_diff_too_short",
            "n_effective": int(len(idx2)),
            "preprocess": {"src": m1, "dst": m2}
        })
        continue

    # 2系列のデータフレーム化 → 標準化
    df = pd.DataFrame({"x": x_tr, "y": y_tr}).dropna()
    df = zscore(df)

    # VAR(1) を基本とし、AIC/BICが1を選ぶか確認（Tが小さいので最大でも2に制限）
    try:
        model = VAR(df)
        sel = model.select_order(maxlags=min(2, max(1, len(df)//3)))  # ごく控えめに
        p = int(sel.selected_orders.get("bic", 1) or 1)
        p = max(1, min(p, 2))
        res = model.fit(p)
        # x→y と y→x のグレンジャー
        g_xy = res.test_causality("y", ["x"], kind="f")
        g_yx = res.test_causality("x", ["y"], kind="f")
        results.append({
            "edge": {"from": src, "to": dst},
            "status": "ok",
            "n_raw_common": 10,
            "n_effective": int(len(df)),
            "lag_selected": p,
            "preprocess": {"from": m1, "to": m2},
            "adf_stationary_hint": {"x": adf_ok(df["x"]), "y": adf_ok(df["y"])},
            "granger": {
                "from_to": {"x_to_y_pvalue": float(g_xy.pvalue)},
                "to_from": {"y_to_x_pvalue": float(g_yx.pvalue)}
            },
            "ic": {
                "aic": float(res.aic),
                "bic": float(res.bic)
            }
        })
    except Exception as e:
        results.append({
            "edge": {"from": src, "to": dst},
            "status": "failed",
            "error": f"{type(e).__name__}: {e}"
        })

# 保存
with open(OUT_PATH, "w", encoding="utf-8") as f:
    json.dump({"results": results}, f, ensure_ascii=False, indent=2)

print(f"[DONE] Granger結果: {OUT_PATH}")
print("  対象エッジ数:", len(edges))
print("  実行済み:", sum(1 for r in results if r["status"] == "ok"))
print("  スキップ:", sum(1 for r in results if r["status"].startswith("skip")))
print("  失敗:", sum(1 for r in results if r["status"] == "failed"))